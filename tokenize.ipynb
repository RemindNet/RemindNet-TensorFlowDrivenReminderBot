{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('set_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>task_indices</th>\n",
       "      <th>date_indices</th>\n",
       "      <th>time_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>schedule a meeting with john tomorrow at 2 pm</td>\n",
       "      <td>['O', 'O', 'B-task_detail', 'I-task_detail', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'B-date', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remind me to buy groceries on friday at 6 pm</td>\n",
       "      <td>['O', 'O', 'O', 'B-task_detail', 'I-task_detai...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>schedule a call with sarah for next monday at ...</td>\n",
       "      <td>['O', 'O', 'B-task_detail', 'I-task_detail', '...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'I-da...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>set a reminder to submit the project report by...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'B-task_detail', 'I-task_...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>remind me to water the plants on sunday morning</td>\n",
       "      <td>['O', 'O', 'O', 'B-task_detail', 'I-task_detai...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-date', ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0      schedule a meeting with john tomorrow at 2 pm   \n",
       "1       remind me to buy groceries on friday at 6 pm   \n",
       "2  schedule a call with sarah for next monday at ...   \n",
       "3  set a reminder to submit the project report by...   \n",
       "4    remind me to water the plants on sunday morning   \n",
       "\n",
       "                                        task_indices  \\\n",
       "0  ['O', 'O', 'B-task_detail', 'I-task_detail', '...   \n",
       "1  ['O', 'O', 'O', 'B-task_detail', 'I-task_detai...   \n",
       "2  ['O', 'O', 'B-task_detail', 'I-task_detail', '...   \n",
       "3  ['O', 'O', 'O', 'O', 'B-task_detail', 'I-task_...   \n",
       "4  ['O', 'O', 'O', 'B-task_detail', 'I-task_detai...   \n",
       "\n",
       "                                        date_indices  \\\n",
       "0  ['O', 'O', 'O', 'O', 'O', 'B-date', 'O', 'O', ...   \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'O', ...   \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'I-da...   \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-date', ...   \n",
       "\n",
       "                                        time_indices  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', ...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ti...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ti...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_indices = data['task_indices'].tolist()\n",
    "time_indices = data['time_indices'].tolist()\n",
    "date_indices = data['date_indices'].tolist()\n",
    "input_statements = data['input'].tolist()\n",
    "\n",
    "task_indices = [eval(elem) for elem in task_indices]\n",
    "time_indices = [eval(elem) for elem in time_indices]\n",
    "date_indices = [eval(elem) for elem in date_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(input_statements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample input statements and corresponding labels\n",
    "# input_statements = [\n",
    "#     \"schedule a meeting with john tomorrow at 2 pm\",\n",
    "#     \"remind me to buy groceries on friday at 6 pm\",\n",
    "#     \"schedule a call with sarah for next monday at 10 am\",\n",
    "#     # Add more input statements as needed\n",
    "# ]\n",
    "\n",
    "# task_indices = [\n",
    "#     ['O', 'O', 'B-task_detail', 'I-task_detail', 'I-task_detail', 'O', 'O', 'O', 'O'],\n",
    "#     ['O', 'O', 'O', 'B-task_detail', 'I-task_detail', 'O', 'O', 'O', 'O', 'O'],\n",
    "#     ['O', 'O', 'B-task_detail', 'I-task_detail', 'I-task_detail', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
    "#     # Add more lists of task indices for other input statements\n",
    "# ]\n",
    "\n",
    "# date_indices = [\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'B-date', 'O', 'O', 'O'],\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'O', 'O', 'O'],\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'O', 'B-date', 'I-date', 'O', 'O', 'O'],\n",
    "#     # Add more lists of date indices for other input statements\n",
    "# ]\n",
    "\n",
    "# time_indices = [\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', 'I-time'],\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', 'I-time'],\n",
    "#     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-time', 'I-time'],\n",
    "#     # Add more lists of time indices for other input statements\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input statements and encode labels\n",
    "encoded_inputs = tokenizer(input_statements, add_special_tokens=True, padding='longest', truncation=True,return_tensors=\"tf\")\n",
    "\n",
    "# Convert labels to integer indices\n",
    "label_to_index = {\"O\": 0, \"B-task_detail\": 1, \"I-task_detail\": 2, \"B-date\": 3, \"I-date\": 4, \"B-time\": 5, \"I-time\": 6}\n",
    "# Separate encoding for task, date, and time entities\n",
    "encoded_labels_task = [[label_to_index[label] for label in task] for task in task_indices]\n",
    "# encoded_labels_date = [[label_to_index[label] for label in date] for date in date_indices]\n",
    "# encoded_labels_time = [[label_to_index[label] for label in time] for time in time_indices]\n",
    "\n",
    "\n",
    "# Convert encoded labels to TensorFlow tensors\n",
    "# encoded_labels_task = tf.convert_to_tensor(encoded_labels_task)\n",
    "# encoded_labels_date = tf.convert_to_tensor(encoded_labels_date)\n",
    "# encoded_labels_time = tf.convert_to_tensor(encoded_labels_time)\n",
    "\n",
    "\n",
    "# print(\"Encoded Inputs:\", len(encoded_inputs['input_ids']))\n",
    "\n",
    "# print(\"Encoded Labels:\", encoded_labels_date[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_inputs['input_ids']),len(encoded_labels_task)) #,len(encoded_labels_date),len(encoded_labels_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "max_length = encoded_inputs['input_ids'][0].shape[0]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad labels with \"O\" to make them rectangular\n",
    "def pad_labels(labels, max_length):\n",
    "    return labels + ['O'] * (max_length - len(labels))\n",
    "\n",
    "# Pad labels for each entity type\n",
    "padded_task_indices = [pad_labels(task, max_length) for task in task_indices]\n",
    "# padded_date_indices = [pad_labels(date, max_length) for date in date_indices]\n",
    "# padded_time_indices = [pad_labels(time, max_length) for time in time_indices]\n",
    "\n",
    "# Encode padded labels separately for each entity type\n",
    "encoded_labels_task = [[label_to_index[label] for label in task] for task in padded_task_indices]\n",
    "# encoded_labels_date = [[label_to_index[label] for label in date] for date in padded_date_indices]\n",
    "# encoded_labels_time = [[label_to_index[label] for label in time] for time in padded_time_indices]\n",
    "\n",
    "# Convert encoded labels to TensorFlow tensors\n",
    "encoded_labels_task = tf.constant(encoded_labels_task)\n",
    "# encoded_labels_date = tf.constant(encoded_labels_date)\n",
    "# encoded_labels_time = tf.constant(encoded_labels_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define input layers\n",
    "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "\n",
    "# Get BERT outputs\n",
    "bert_outputs = bert_model(input_ids)[0]  # Output from BERT's last layer\n",
    "\n",
    "# Define classification heads for each entity type\n",
    "num_labels_task = 3  # \"O\", \"B-task\", \"I-task\"\n",
    "num_labels_date = 3  # \"O\", \"B-date\", \"I-date\"\n",
    "num_labels_time = 3  # \"O\", \"B-time\", \"I-time\"\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Task classification head\n",
    "task_logits = tf.keras.layers.Dense(num_labels_task, activation=None, name=\"task_logits\")(bert_outputs)\n",
    "task_output = tf.keras.layers.Activation(\"softmax\", name=\"task_output\")(task_logits)\n",
    "\n",
    "# # Date classification head\n",
    "# date_logits = tf.keras.layers.Dense(num_labels_date, activation=None, name=\"date_logits\")(bert_outputs)\n",
    "# date_output = tf.keras.layers.Activation(\"softmax\", name=\"date_output\")(date_logits)\n",
    "\n",
    "# # Time classification head\n",
    "# time_logits = tf.keras.layers.Dense(num_labels_time, activation=None, name=\"time_logits\")(bert_outputs)\n",
    "# time_output = tf.keras.layers.Activation(\"softmax\", name=\"time_output\")(time_logits)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model(inputs=input_ids, outputs=task_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_ids (InputLayer)      [(None, None)]            0         \n",
      "                                                                 \n",
      " tf_bert_model_2 (TFBertMod  TFBaseModelOutputWithPo   109482240 \n",
      " el)                         olingAndCrossAttentions             \n",
      "                             (last_hidden_state=(Non             \n",
      "                             e, None, 768),                      \n",
      "                              pooler_output=(None, 7             \n",
      "                             68),                                \n",
      "                              past_key_values=None,              \n",
      "                             hidden_states=None, att             \n",
      "                             entions=None, cross_att             \n",
      "                             entions=None)                       \n",
      "                                                                 \n",
      " task_logits (Dense)         (None, None, 3)           2307      \n",
      "                                                                 \n",
      " task_output (Activation)    (None, None, 3)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109484547 (417.65 MB)\n",
      "Trainable params: 109484547 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 149ms/step - loss: 0.4788 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 1s 142ms/step - loss: 0.4749 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 0.4782 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.4725 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 1s 137ms/step - loss: 0.4755 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.4738 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.4759 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.4754 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.4862 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.4782 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.4732 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 0.4794 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 1s 148ms/step - loss: 0.4823 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 1s 150ms/step - loss: 0.4764 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 1s 128ms/step - loss: 0.4795 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.4757 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 1s 131ms/step - loss: 0.4752 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.4832 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 1s 144ms/step - loss: 0.4768 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 1s 149ms/step - loss: 0.4987 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 1s 157ms/step - loss: 0.4997 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.5096 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.4849 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.4806 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.4766 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 1s 129ms/step - loss: 0.4772 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.4780 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.4788 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 1s 134ms/step - loss: 0.4826 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 1s 133ms/step - loss: 0.4789 - sparse_categorical_accuracy: 0.8696\n",
      "Epoch 31/50\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4863 - sparse_categorical_accuracy: 0.8655"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_labels_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:138\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:391\u001b[0m, in \u001b[0;36mFunctionType.unpack_inputs\u001b[0;34m(self, bound_parameters)\u001b[0m\n\u001b[1;32m    388\u001b[0m flat \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sorted_parameters:\n\u001b[1;32m    390\u001b[0m   flat\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 391\u001b[0m       \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbound_parameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m   )\n\u001b[1;32m    394\u001b[0m dealiased_inputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    395\u001b[0m ids_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:254\u001b[0m, in \u001b[0;36mTypeSpec.to_tensors\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See TraceType base class for details. Do not override.\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m tensors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    252\u001b[0m nest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m spec, v: tensors\u001b[38;5;241m.\u001b[39mextend(spec\u001b[38;5;241m.\u001b[39mto_tensors(v)),\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_component_specs\u001b[49m,\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_components(value))\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensors\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:949\u001b[0m, in \u001b[0;36mIteratorSpec._component_specs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_component_specs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 949\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource\u001b[49m\u001b[43m)\u001b[49m,)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:867\u001b[0m, in \u001b[0;36mDenseSpec.__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a TensorSpec.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[38;5;124;03m    not convertible to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m tensor_shape\u001b[38;5;241m.\u001b[39mTensorShape(shape)\n\u001b[0;32m--> 867\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/dtypes.py:793\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Ensure no collisions.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_ANY_TO_TF) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mlen\u001b[39m(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m [_INTERN_TABLE, _STRING_TO_TF, _PYTHON_TO_TF, _NP_TO_TF])\n\u001b[0;32m--> 793\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtypes.as_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    794\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mas_dtype\u001b[39m(type_value):\n\u001b[1;32m    795\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `type_value` to a `tf.DType`.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \n\u001b[1;32m    797\u001b[0m \u001b[38;5;124;03m  Inputs can be existing `tf.DType` objects, a [`DataType`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03m    TypeError: If `type_value` cannot be converted to a `DType`.\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m    825\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_value, DType):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    encoded_inputs['input_ids'],\n",
    "    encoded_labels_task,\n",
    "    epochs=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
